{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Comment analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Online store launches a new service. Now users can edit and supplement product descriptions, just like in wiki communities. The store needs a tool that will look for toxic comments and submit them for moderation.\n",
    "\n",
    "Train the model to classify comments as positive or negative. At your disposal is a dataset with markup on the toxicity of edits.\n",
    "\n",
    "Build a model with a quality metric *F1* of at least 0.75.\n",
    "\n",
    "**Instructions for the implementation of the project**\n",
    "\n",
    "1. Download and prepare data.\n",
    "2. Train different models.\n",
    "3. Draw conclusions.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loading and processing data "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from pymystem3 import Mystem\n",
    "import nltk \n",
    "from nltk.corpus import stopwords as nltk_stopwords\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import f1_score, accuracy_score, confusion_matrix, recall_score, precision_score\n",
    "import re \n",
    "import warnings\n",
    "warnings.simplefilter(action='ignore', category=Warning)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\pavel\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "nltk.download('stopwords')\n",
    "w_tokenizer = nltk.tokenize.WhitespaceTokenizer()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Initial analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('datasets/toxic_comments.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>toxic</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>COCKSUCKER BEFORE YOU PISS AROUND ON MY WORK</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>Hey... what is it..\\n@ | talk .\\nWhat is it......</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>Bye! \\n\\nDon't look, come or think of comming ...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>42</th>\n",
       "      <td>You are gay or antisemmitian? \\n\\nArchangel WH...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>43</th>\n",
       "      <td>FUCK YOUR FILTHY MOTHER IN THE ASS, DRY!</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>44</th>\n",
       "      <td>I'm Sorry \\n\\nI'm sorry I screwed around with ...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>51</th>\n",
       "      <td>GET FUCKED UP. GET FUCKEEED UP.  GOT A DRINK T...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>55</th>\n",
       "      <td>Stupid peace of shit stop deleting my stuff as...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>56</th>\n",
       "      <td>=Tony Sidaway is obviously a fistfuckee. He lo...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>58</th>\n",
       "      <td>My Band Page's deletion. You thought I was gon...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>59</th>\n",
       "      <td>Why can't you believe how fat Artie is? Did yo...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>65</th>\n",
       "      <td>All of my edits are good.  Cunts like you who ...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>79</th>\n",
       "      <td>Hi! I am back again!\\nLast warning!\\nStop undo...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>86</th>\n",
       "      <td>Would you both shut up, you don't run wikipedi...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>105</th>\n",
       "      <td>A pair of jew-hating weiner nazi schmucks.</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>151</th>\n",
       "      <td>\"\\n\\nSORRY PUCK BUT NO ONE EVER SAID DICK WAS ...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>159</th>\n",
       "      <td>\"\\n\\nUNBLOCK ME OR I'LL GET MY LAWYERS ON TO Y...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>168</th>\n",
       "      <td>You should be fired, you're a moronic wimp who...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>176</th>\n",
       "      <td>I think that your a Fagget get a oife and burn...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>179</th>\n",
       "      <td>REPLY ABOVE:\\nThat was me, loser. The UN defin...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                  text  toxic\n",
       "6         COCKSUCKER BEFORE YOU PISS AROUND ON MY WORK      1\n",
       "12   Hey... what is it..\\n@ | talk .\\nWhat is it......      1\n",
       "16   Bye! \\n\\nDon't look, come or think of comming ...      1\n",
       "42   You are gay or antisemmitian? \\n\\nArchangel WH...      1\n",
       "43            FUCK YOUR FILTHY MOTHER IN THE ASS, DRY!      1\n",
       "44   I'm Sorry \\n\\nI'm sorry I screwed around with ...      1\n",
       "51   GET FUCKED UP. GET FUCKEEED UP.  GOT A DRINK T...      1\n",
       "55   Stupid peace of shit stop deleting my stuff as...      1\n",
       "56   =Tony Sidaway is obviously a fistfuckee. He lo...      1\n",
       "58   My Band Page's deletion. You thought I was gon...      1\n",
       "59   Why can't you believe how fat Artie is? Did yo...      1\n",
       "65   All of my edits are good.  Cunts like you who ...      1\n",
       "79   Hi! I am back again!\\nLast warning!\\nStop undo...      1\n",
       "86   Would you both shut up, you don't run wikipedi...      1\n",
       "105         A pair of jew-hating weiner nazi schmucks.      1\n",
       "151  \"\\n\\nSORRY PUCK BUT NO ONE EVER SAID DICK WAS ...      1\n",
       "159  \"\\n\\nUNBLOCK ME OR I'LL GET MY LAWYERS ON TO Y...      1\n",
       "168  You should be fired, you're a moronic wimp who...      1\n",
       "176  I think that your a Fagget get a oife and burn...      1\n",
       "179  REPLY ABOVE:\\nThat was me, loser. The UN defin...      1"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.query('toxic==1').head(20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 159571 entries, 0 to 159570\n",
      "Data columns (total 2 columns):\n",
      " #   Column  Non-Null Count   Dtype \n",
      "---  ------  --------------   ----- \n",
      " 0   text    159571 non-null  object\n",
      " 1   toxic   159571 non-null  int64 \n",
      "dtypes: int64(1), object(1)\n",
      "memory usage: 2.4+ MB\n"
     ]
    }
   ],
   "source": [
    "df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>toxic</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>159571.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>0.101679</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>0.302226</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "               toxic\n",
       "count  159571.000000\n",
       "mean        0.101679\n",
       "std         0.302226\n",
       "min         0.000000\n",
       "25%         0.000000\n",
       "50%         0.000000\n",
       "75%         0.000000\n",
       "max         1.000000"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Processing data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clear_text(text):\n",
    "    x = re.sub(r'[^a-zA-Z ]', ' ', text) \n",
    "    return \" \".join(x.split())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "lemmatizer = nltk.stem.WordNetLemmatizer()\n",
    "def lemmatize_text(text):\n",
    "    return ' '.join([lemmatizer.lemmatize(w) for w in w_tokenizer.tokenize(text)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['text'] = df['text'].apply(clear_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['text_lemmatized'] = df['text'].apply(lemmatize_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>toxic</th>\n",
       "      <th>text_lemmatized</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Explanation Why the edits made under my userna...</td>\n",
       "      <td>0</td>\n",
       "      <td>Explanation Why the edits made under my userna...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>D aww He matches this background colour I m se...</td>\n",
       "      <td>0</td>\n",
       "      <td>D aww He match this background colour I m seem...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Hey man I m really not trying to edit war It s...</td>\n",
       "      <td>0</td>\n",
       "      <td>Hey man I m really not trying to edit war It s...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>More I can t make any real suggestions on impr...</td>\n",
       "      <td>0</td>\n",
       "      <td>More I can t make any real suggestion on impro...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>You sir are my hero Any chance you remember wh...</td>\n",
       "      <td>0</td>\n",
       "      <td>You sir are my hero Any chance you remember wh...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>159566</th>\n",
       "      <td>And for the second time of asking when your vi...</td>\n",
       "      <td>0</td>\n",
       "      <td>And for the second time of asking when your vi...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>159567</th>\n",
       "      <td>You should be ashamed of yourself That is a ho...</td>\n",
       "      <td>0</td>\n",
       "      <td>You should be ashamed of yourself That is a ho...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>159568</th>\n",
       "      <td>Spitzer Umm theres no actual article for prost...</td>\n",
       "      <td>0</td>\n",
       "      <td>Spitzer Umm there no actual article for prosti...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>159569</th>\n",
       "      <td>And it looks like it was actually you who put ...</td>\n",
       "      <td>0</td>\n",
       "      <td>And it look like it wa actually you who put on...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>159570</th>\n",
       "      <td>And I really don t think you understand I came...</td>\n",
       "      <td>0</td>\n",
       "      <td>And I really don t think you understand I came...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>159571 rows × 3 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                     text  toxic  \\\n",
       "0       Explanation Why the edits made under my userna...      0   \n",
       "1       D aww He matches this background colour I m se...      0   \n",
       "2       Hey man I m really not trying to edit war It s...      0   \n",
       "3       More I can t make any real suggestions on impr...      0   \n",
       "4       You sir are my hero Any chance you remember wh...      0   \n",
       "...                                                   ...    ...   \n",
       "159566  And for the second time of asking when your vi...      0   \n",
       "159567  You should be ashamed of yourself That is a ho...      0   \n",
       "159568  Spitzer Umm theres no actual article for prost...      0   \n",
       "159569  And it looks like it was actually you who put ...      0   \n",
       "159570  And I really don t think you understand I came...      0   \n",
       "\n",
       "                                          text_lemmatized  \n",
       "0       Explanation Why the edits made under my userna...  \n",
       "1       D aww He match this background colour I m seem...  \n",
       "2       Hey man I m really not trying to edit war It s...  \n",
       "3       More I can t make any real suggestion on impro...  \n",
       "4       You sir are my hero Any chance you remember wh...  \n",
       "...                                                   ...  \n",
       "159566  And for the second time of asking when your vi...  \n",
       "159567  You should be ashamed of yourself That is a ho...  \n",
       "159568  Spitzer Umm there no actual article for prosti...  \n",
       "159569  And it look like it wa actually you who put on...  \n",
       "159570  And I really don t think you understand I came...  \n",
       "\n",
       "[159571 rows x 3 columns]"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "stop_words = set(nltk_stopwords.words('english'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "count_tf_idf = TfidfVectorizer(stop_words=stop_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['Uppercase'] = df['text_lemmatized'].str.count(r'[A-Z]')\n",
    "df['Lowercase'] = df['text_lemmatized'].str.count(r'[a-z]')\n",
    "df['share_of_capital'] = df['Uppercase']/(df['Lowercase']+df['Uppercase'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df.drop(['text', 'Uppercase', 'Lowercase'], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>toxic</th>\n",
       "      <th>text_lemmatized</th>\n",
       "      <th>share_of_capital</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>Explanation Why the edits made under my userna...</td>\n",
       "      <td>0.084158</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>D aww He match this background colour I m seem...</td>\n",
       "      <td>0.112676</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0</td>\n",
       "      <td>Hey man I m really not trying to edit war It s...</td>\n",
       "      <td>0.021505</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0</td>\n",
       "      <td>More I can t make any real suggestion on impro...</td>\n",
       "      <td>0.023158</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0</td>\n",
       "      <td>You sir are my hero Any chance you remember wh...</td>\n",
       "      <td>0.040000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>159566</th>\n",
       "      <td>0</td>\n",
       "      <td>And for the second time of asking when your vi...</td>\n",
       "      <td>0.008929</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>159567</th>\n",
       "      <td>0</td>\n",
       "      <td>You should be ashamed of yourself That is a ho...</td>\n",
       "      <td>0.030303</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>159568</th>\n",
       "      <td>0</td>\n",
       "      <td>Spitzer Umm there no actual article for prosti...</td>\n",
       "      <td>0.064516</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>159569</th>\n",
       "      <td>0</td>\n",
       "      <td>And it look like it wa actually you who put on...</td>\n",
       "      <td>0.022472</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>159570</th>\n",
       "      <td>0</td>\n",
       "      <td>And I really don t think you understand I came...</td>\n",
       "      <td>0.030303</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>159571 rows × 3 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "        toxic                                    text_lemmatized  \\\n",
       "0           0  Explanation Why the edits made under my userna...   \n",
       "1           0  D aww He match this background colour I m seem...   \n",
       "2           0  Hey man I m really not trying to edit war It s...   \n",
       "3           0  More I can t make any real suggestion on impro...   \n",
       "4           0  You sir are my hero Any chance you remember wh...   \n",
       "...       ...                                                ...   \n",
       "159566      0  And for the second time of asking when your vi...   \n",
       "159567      0  You should be ashamed of yourself That is a ho...   \n",
       "159568      0  Spitzer Umm there no actual article for prosti...   \n",
       "159569      0  And it look like it wa actually you who put on...   \n",
       "159570      0  And I really don t think you understand I came...   \n",
       "\n",
       "        share_of_capital  \n",
       "0               0.084158  \n",
       "1               0.112676  \n",
       "2               0.021505  \n",
       "3               0.023158  \n",
       "4               0.040000  \n",
       "...                  ...  \n",
       "159566          0.008929  \n",
       "159567          0.030303  \n",
       "159568          0.064516  \n",
       "159569          0.022472  \n",
       "159570          0.030303  \n",
       "\n",
       "[159571 rows x 3 columns]"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Conclusions**\n",
    "\n",
    "1. Data uploaded\n",
    "2. We can try to estimate toxic comments by the share of characters with caps\n",
    "3. The proportion of toxic comments - about 10% - may not be enough for a models\n",
    "4. It seems that lemmatization here is not particularly useful (compared to Russian)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "train, test = train_test_split(df, test_size = 0.2, random_state=12)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "corpus = train['text_lemmatized'].values.astype('U')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "tf_idf = count_tf_idf.fit_transform(corpus)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "target_train = train['toxic']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "corpus_test = test['text_lemmatized'].values.astype('U')\n",
    "tf_idf_test = count_tf_idf.transform(corpus_test)\n",
    "target_test = test['toxic']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.9592811932067431\n"
     ]
    }
   ],
   "source": [
    "model = LogisticRegression()\n",
    "model.fit(tf_idf, target_train)\n",
    "predict_train = model.predict(tf_idf)\n",
    "print(model.score(tf_idf, target_train))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.9562901456995143\n",
      "0.7463175122749591\n",
      "[[28468   122]\n",
      " [ 1273  2052]]\n"
     ]
    }
   ],
   "source": [
    "predict_test = model.predict(tf_idf_test)\n",
    "print(model.score(tf_idf_test, target_test))\n",
    "print(f1_score(predict_test, target_test))\n",
    "print(confusion_matrix(target_test, predict_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "toxic\n",
       "0    28590\n",
       "1     3325\n",
       "Name: toxic, dtype: int64"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test.groupby('toxic')['toxic'].count()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The model missed 40% of toxic comments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.9608479037413048\n",
      "0.9429735234215886\n",
      "0.7578499201703034\n",
      "[[27247  1343]\n",
      " [  477  2848]]\n"
     ]
    }
   ],
   "source": [
    "model_l = LogisticRegression(class_weight='balanced')\n",
    "model_l.fit(tf_idf, target_train)\n",
    "predict_train = model_l.predict(tf_idf)\n",
    "print(model_l.score(tf_idf, target_train))\n",
    "predict_test = model_l.predict(tf_idf_test)\n",
    "print(model_l.score(tf_idf_test, target_test))\n",
    "print(f1_score(predict_test, target_test))\n",
    "print(confusion_matrix(target_test, predict_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There are more false positives in this model, but required $f1$ value is acheived "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.9996553236823964\n",
      "0.942754190819364\n",
      "0.6343806283770262\n",
      "[[28503    87]\n",
      " [ 1740  1585]]\n"
     ]
    }
   ],
   "source": [
    "model_f = RandomForestClassifier(class_weight='balanced')\n",
    "model_f.fit(tf_idf, target_train)\n",
    "predict_train = model_f.predict(tf_idf)\n",
    "print(model_f.score(tf_idf, target_train))\n",
    "predict_test = model_f.predict(tf_idf_test)\n",
    "print(model_f.score(tf_idf_test, target_test))\n",
    "print(f1_score(predict_test, target_test))\n",
    "print(confusion_matrix(target_test, predict_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Results**\n",
    "\n",
    "1. 3 models were built\n",
    "2. Two of them are logistic regression, the second one balanced classes\n",
    "3. The random forest model gives a low result. The reason is that I did not change the hyperparameters. But since we have a lot of variables here, it takes too long to build a good tree"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Final model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Score на train: 0.9608400701886319\n",
      "Score на test: 0.9430048566504778\n",
      "F1: 0.7579507651363938\n",
      "[[27248  1342]\n",
      " [  477  2848]]\n"
     ]
    }
   ],
   "source": [
    "model_l = LogisticRegression(class_weight='balanced')\n",
    "model_l.fit(tf_idf, target_train)\n",
    "predict_train = model_l.predict(tf_idf)\n",
    "print(\"Score на train:\",model_l.score(tf_idf, target_train))\n",
    "predict_test = model_l.predict(tf_idf_test)\n",
    "print(\"Score на test:\",model_l.score(tf_idf_test, target_test))\n",
    "print(\"F1:\",f1_score(predict_test, target_test))\n",
    "print(confusion_matrix(target_test, predict_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overall score: 0.9430048566504778\n",
      "Recall: 0.8565413533834586\n",
      "Precision: 0.6797136038186158\n",
      "f1 score: 0.7579507651363938\n"
     ]
    }
   ],
   "source": [
    "print(\"Overall score:\",accuracy_score(target_test, predict_test))\n",
    "print(\"Recall:\",recall_score(target_test, predict_test))\n",
    "print(\"Precision:\",precision_score(target_test, predict_test))\n",
    "print(\"f1 score:\",f1_score(predict_test, target_test))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Example of comments, that were not caught by the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>toxic</th>\n",
       "      <th>text_lemmatized</th>\n",
       "      <th>share_of_capital</th>\n",
       "      <th>pred</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>69591</th>\n",
       "      <td>1</td>\n",
       "      <td>fukk it im goin encyclopedia dramatica</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21861</th>\n",
       "      <td>1</td>\n",
       "      <td>Bullshit Why doe Amy Roloff get her own entry ...</td>\n",
       "      <td>0.065789</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5020</th>\n",
       "      <td>1</td>\n",
       "      <td>give a on them I inserted</td>\n",
       "      <td>0.050000</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>144387</th>\n",
       "      <td>1</td>\n",
       "      <td>oo if you have ANY actual evidence show it You...</td>\n",
       "      <td>0.047945</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28987</th>\n",
       "      <td>1</td>\n",
       "      <td>Al Qaedia is after you</td>\n",
       "      <td>0.111111</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>30237</th>\n",
       "      <td>1</td>\n",
       "      <td>you re so idi o t so st u p id guy fu c k i n ...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>129934</th>\n",
       "      <td>1</td>\n",
       "      <td>Your ongoing effort to vandalize the popper in...</td>\n",
       "      <td>0.023256</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>61143</th>\n",
       "      <td>1</td>\n",
       "      <td>Don t be an as It wa a clear edit conflict And...</td>\n",
       "      <td>0.120690</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>45404</th>\n",
       "      <td>1</td>\n",
       "      <td>Secular Humanism Islam s best friend I know yo...</td>\n",
       "      <td>0.113990</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>55053</th>\n",
       "      <td>1</td>\n",
       "      <td>Your work will be deleted or bastardized wheth...</td>\n",
       "      <td>0.005263</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>107654</th>\n",
       "      <td>1</td>\n",
       "      <td>I am not asking or looking for sympathy and mi...</td>\n",
       "      <td>0.023148</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>39796</th>\n",
       "      <td>1</td>\n",
       "      <td>See the edit reason you muppet</td>\n",
       "      <td>0.040000</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>135039</th>\n",
       "      <td>1</td>\n",
       "      <td>guy a right knobjockey</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>109351</th>\n",
       "      <td>1</td>\n",
       "      <td>April UTC Well I m glad you don t take yoursel...</td>\n",
       "      <td>0.092308</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>101483</th>\n",
       "      <td>1</td>\n",
       "      <td>Conflict of interest You are a person who is d...</td>\n",
       "      <td>0.057592</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>85608</th>\n",
       "      <td>1</td>\n",
       "      <td>Anushirvan Khampalak is just hurted that he is...</td>\n",
       "      <td>0.036364</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>143542</th>\n",
       "      <td>1</td>\n",
       "      <td>The various local government area page only li...</td>\n",
       "      <td>0.022366</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>45626</th>\n",
       "      <td>1</td>\n",
       "      <td>Newsflash journalist fails at using English co...</td>\n",
       "      <td>0.022901</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>152772</th>\n",
       "      <td>1</td>\n",
       "      <td>SimCopter Shenanigans Hiya I originally follow...</td>\n",
       "      <td>0.050562</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>100522</th>\n",
       "      <td>1</td>\n",
       "      <td>I call and is gone Swedish duck of memory</td>\n",
       "      <td>0.060606</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        toxic                                    text_lemmatized  \\\n",
       "69591       1             fukk it im goin encyclopedia dramatica   \n",
       "21861       1  Bullshit Why doe Amy Roloff get her own entry ...   \n",
       "5020        1                          give a on them I inserted   \n",
       "144387      1  oo if you have ANY actual evidence show it You...   \n",
       "28987       1                             Al Qaedia is after you   \n",
       "30237       1  you re so idi o t so st u p id guy fu c k i n ...   \n",
       "129934      1  Your ongoing effort to vandalize the popper in...   \n",
       "61143       1  Don t be an as It wa a clear edit conflict And...   \n",
       "45404       1  Secular Humanism Islam s best friend I know yo...   \n",
       "55053       1  Your work will be deleted or bastardized wheth...   \n",
       "107654      1  I am not asking or looking for sympathy and mi...   \n",
       "39796       1                     See the edit reason you muppet   \n",
       "135039      1                             guy a right knobjockey   \n",
       "109351      1  April UTC Well I m glad you don t take yoursel...   \n",
       "101483      1  Conflict of interest You are a person who is d...   \n",
       "85608       1  Anushirvan Khampalak is just hurted that he is...   \n",
       "143542      1  The various local government area page only li...   \n",
       "45626       1  Newsflash journalist fails at using English co...   \n",
       "152772      1  SimCopter Shenanigans Hiya I originally follow...   \n",
       "100522      1          I call and is gone Swedish duck of memory   \n",
       "\n",
       "        share_of_capital  pred  \n",
       "69591           0.000000     0  \n",
       "21861           0.065789     0  \n",
       "5020            0.050000     0  \n",
       "144387          0.047945     0  \n",
       "28987           0.111111     0  \n",
       "30237           0.000000     0  \n",
       "129934          0.023256     0  \n",
       "61143           0.120690     0  \n",
       "45404           0.113990     0  \n",
       "55053           0.005263     0  \n",
       "107654          0.023148     0  \n",
       "39796           0.040000     0  \n",
       "135039          0.000000     0  \n",
       "109351          0.092308     0  \n",
       "101483          0.057592     0  \n",
       "85608           0.036364     0  \n",
       "143542          0.022366     0  \n",
       "45626           0.022901     0  \n",
       "152772          0.050562     0  \n",
       "100522          0.060606     0  "
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test['pred'] = predict_test.tolist()\n",
    "test.query('toxic == 1 and pred == 0').head(20)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Conclusions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Conclusions**\n",
    "\n",
    "1. The resulting model gives 85% of toxic comments for moderation\n",
    "2. At the same time, she captures a lot of normal comments - about 1 out of 3 comments that model marks as toxic are false positives\n",
    "3. Hypothesis: in order to improve accuracy, we can additionaly take a look at:\n",
    "* high proportion of letters in caps\n",
    "* use a  dictionary check of the curse words\n",
    "* check the ammount of question and exclamation marks"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.8.8 ('base')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "b7d258fa581379cc616dd21d18b044438587cad8c2fc6656384f324d1def7e8d"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
